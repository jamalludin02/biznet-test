# AI Knowledge Chat Frontend

Vue.js frontend application for AI chat interface using Ollama with Gemma3:1b model.

## ğŸš€ Quick Start

### Using Docker (Recommended)
```bash
# From project root
docker-compose up -d ai-knowledge-frontend
```

### Local Development
```bash
cd ai-knowledge
npm install
npm run dev
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vue.js App    â”‚    â”‚   Ollama AI     â”‚
â”‚   (Port 3000)   â”‚â—„â”€â”€â–ºâ”‚   (Port 11434)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                       â–²
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Nginx Proxy    â”‚
         â”‚   (Port 80)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Structure

```
ai-knowledge/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.vue        # Main chat component
â”‚   â”œâ”€â”€ main.js        # Vue entry point
â”‚   â””â”€â”€ style.css      # Tailwind styles
â”œâ”€â”€ Dockerfile         # Frontend container
â””â”€â”€ package.json       # Dependencies
```

## ğŸŒ Access

- **Local Development**: http://localhost:3000
- **Docker**: http://localhost:3000

## ğŸ› ï¸ Development

### Prerequisites
- Node.js 18+ (for local development)
- Ollama service running on port 11434

### Commands
```bash
npm install          # Install dependencies
npm run dev         # Start development server
npm run build       # Build for production
npm run preview     # Preview production build
```

## ğŸ”— Dependencies

- **Vue.js 3** - Frontend framework
- **Tailwind CSS** - Styling
- **@vueuse/core** - Vue composition utilities

## ğŸ“ Notes

- Requires Ollama service running on `localhost:11434`
- Model `gemma3:1b` must be available in Ollama
- CORS is handled by direct API calls to Ollama 